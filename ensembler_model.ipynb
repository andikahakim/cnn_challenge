{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda installed! Running on GPU!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# this is the same function from the lecture; included here to easily set the seed for random functions\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True  #uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = True\n",
    "\n",
    "    return True\n",
    "\n",
    "# set the device to GPU if available:\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")\n",
    "    \n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "out_path = './out'\n",
    "\n",
    "# check if we have the output working directory, if not then create it\n",
    "if not os.path.isdir(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    \n",
    "print(os.listdir(out_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- In this notebook we will construct the Ensembler that was used to make the final submission for our group project.\n",
    "\n",
    "- This ensembler is based on a weighted vote from the models that were deemed to provide good performance and variant architectures\n",
    "\n",
    "- If we had been given more time, we would have constructed an ensemble classifier, with a prediction derived from *per class* weights. Since we were short on time, we applied a weighting based on overall accuracy. This nonetheless gave very good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Data\n",
    "\n",
    "- In this section we process the input test data to get it ready for our model\n",
    "\n",
    "- This consists of normalizing the test data based on our training data statistics. The normalized sample, $\\hat x$ is given by:\n",
    "\n",
    "$$ \\hat x = \\frac{x - \\bar X}{\\sigma_X}$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\bar x: \\text{a single sample image}\\\\\n",
    "&\\bar X: \\text{mean for all the training data, per RGB channel}\\\\\n",
    "&\\sigma_X: \\text{standard deviation for all the training data, per RGB channel}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "- The ***CustomImageTensorDataset*** function, which was used through the project, is used here to apply the normalization transformation to the samples in the test data\n",
    "\n",
    "\n",
    "- The ***ImageFolderWithPaths*** function is used to get the images including their corresponding filenames. This allows the construction of the csv meeting the required submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples (expect 100000): 100000\n",
      "number of training set classes (expect 200): 200\n",
      "number of test classes (expect 1): 2\n",
      "number of test images (expect 10000): 10001\n"
     ]
    }
   ],
   "source": [
    "# define a custom image dataset class - this will be reused whenever we wish to load data into a dataloader\n",
    "class CustomImageTensorDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Tensor): A tensor containing the data e.g. images\n",
    "            targets (Tensor): A tensor containing all the labels\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample, label = self.data[idx], self.targets[idx]        \n",
    "        sample = torch.from_numpy(sample).permute(2, 0, 1)/255.\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "    \n",
    "# this is needed for the test function below. It loads the test dataset correctly\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that also returns image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    Inspiration for this function from https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "    \"\"\"\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "train_path = './train/'\n",
    "test_path = './test/'\n",
    "\n",
    "# grab the dataset structure (this does not read files in, rather it creates a map of files and labels)\n",
    "train_data = ImageFolder(train_path, transform=None)\n",
    "\n",
    "# check that we have the files in the right place\n",
    "print('number of training samples (expect 100000):', len(train_data))\n",
    "print('number of training set classes (expect 200):', len(os.listdir(train_path)))\n",
    "print('number of test classes (expect 1):', len(os.listdir(test_path)))\n",
    "print('number of test images (expect 10000):', len(os.listdir(test_path+'images')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means: [0.48024579 0.44807218 0.39754775] stdevs:  [0.2301945  0.22647534 0.2261424 ]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(data_set):\n",
    "    \"\"\"\n",
    "    This function gets the normalization statistics from a given data set.\n",
    "    It was inspired from the discussion at https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949\n",
    "    Args:\n",
    "        data_set (torch.utils.data.Dataset): This is the data_set from which we wish to derive the\n",
    "            normalization statistics\n",
    "        \n",
    "    Returns:\n",
    "        X_set (np.array): An array containing all the input images in a combined array\n",
    "        y_set (np.array): An array containing all the corresponding inputs in a combined array\n",
    "        meanRGB/n (np.array): An array containing the means per RGB channels\n",
    "        stdRGB/n (np.array): An array containing the standard deviations per RGB channels\n",
    "        \n",
    "    \"\"\"\n",
    "    n = len(data_set)\n",
    "    X_set, y_set = [], []    \n",
    "    meanRGB = np.array([0., 0., 0.])\n",
    "    stdRGB = np.array([0., 0., 0.])\n",
    "\n",
    "    # Here we loop over each image, update the mean and standard deviation\n",
    "    for i, (img, label) in enumerate(data_set):\n",
    "        X = np.array(img)\n",
    "        X_set.append(X)\n",
    "        y_set.append(label)\n",
    "        meanRGB += [(X[:, :, i]/255.).mean() for i in range(3)]\n",
    "        stdRGB += [(X[:, :, i]/255.).std() for i in range(3)]\n",
    "\n",
    "    # Return the combined image and class arrays, and then the averaged mean and standard deviation\n",
    "    return np.array(X_set), np.array(y_set), meanRGB/n, stdRGB/n\n",
    "    \n",
    "X_train, y_train, means, stdevs = get_stats(train_data)\n",
    "print('means:', means, 'stdevs: ', stdevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# # resize the images so they are the right size for out model architectures\n",
    "# train_transform = Compose([\n",
    "#     Resize(224),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=means, std=stdevs)\n",
    "# ])\n",
    "\n",
    "# resize and normalize the test data\n",
    "test_transform = Compose([\n",
    "    Resize(224),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=means, std=stdevs)\n",
    "])\n",
    "\n",
    "# train_dataset = CustomImageTensorDataset(X_train, y_train, transform=train_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# transform and load test data\n",
    "test_data = ImageFolderWithPaths(test_path, transform=test_transform)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data loader has the correct length given our batch size\n",
    "assert len(test_loader) == 10000/test_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Used in The Ensembler Prediction\n",
    "\n",
    "- Below is a list of how to load and ensemble the different models. There is also a demonstration of how to how to use the ***modified_test*** function to make individual predictions\n",
    "\n",
    "\n",
    "- The ***modified_test*** function is used to make predictions on an input dataset. Where it differs from the ***test*** function used elsewhere in the project is that it also returns the log softmax values too. This is to enable it to be used to construct a weighted prediction based on all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader must be with paths\n",
    "def modified_test(model, data_loader):\n",
    "    \"\"\"\n",
    "    Explanation:\n",
    "        This function makes predictions on the test data from a single model\n",
    "    Args:\n",
    "        model (Tensor): A pytorch neural net model\n",
    "        data_loader (DataLoader): A pytorch  dataloader for the test data    \n",
    "    Returns:\n",
    "        log_probs (np.array): An array containing the log-likelihood from the softmax classifier\n",
    "        y_preds (np.array): An array  containing the class label predictions based on the softmax classifier\n",
    "        file_names: (np.array): An array containing the filenames; this is used to construct the kaggle submission\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    log_probs, y_preds, file_names = [], [], []\n",
    "    for X, y, file_paths in data_loader:\n",
    "        with torch.no_grad():\n",
    "            # make sure we can utilize the GPU if available\n",
    "            X, y = torch.from_numpy(np.array(X)).to(device), y.to(device)\n",
    "            \n",
    "            # forward pass through the model\n",
    "            a2 = model(X)\n",
    "            \n",
    "            # we will return this too so as to be able to work out a proper weighted average\n",
    "            log_prob = F.log_softmax(a2, dim=1)\n",
    "\n",
    "            # make the predictions based off our model\n",
    "            y_pred = log_prob.max(1)[1]\n",
    "            \n",
    "            # fix filenames\n",
    "            img_names = [name.split('/')[-1] for name in file_paths]\n",
    "            \n",
    "            # set up the return arrays\n",
    "            log_probs.append(log_prob.cpu().numpy())\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            file_names.extend(img_names)\n",
    "\n",
    "    return np.concatenate(log_probs, 0), np.concatenate(y_preds, 0), np.array(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet18\n",
    "\n",
    "This is the first model we submitted to Kaggle. It got a score of **0.70671** on the public leaderboard. We did not use this model in the final construction, but it gives an example of how to make a prediction using the ***modified_test*** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18_ = models.resnet18(pretrained=True)\n",
    "\n",
    "# adjust the architecture for our data set\n",
    "num_ftrs = resnet18_.fc.in_features\n",
    "resnet18_.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# utilize GPU if possible\n",
    "resnet18_ = resnet18_.to(device)\n",
    "\n",
    "# load the saved weights\n",
    "resnet18_.load_state_dict(torch.load('RESNET_DROPOUT.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using resnet18\n",
    "resnet18_probs, resnet18_preds, resnet18_file_names = modified_test(resnet18_, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide Resnet 101-2\n",
    "\n",
    "This is the second model architecture we submitted to Kaggle. This got a score of **0.81555** on the public leaderboard when we trained it on a subset of the data. It got a score of **0.84109** once trained on the whole training data set. It was included in the ensembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_resnet101_2_ = models.wide_resnet101_2(pretrained=True).to(device)\n",
    "\n",
    "# adjust the architecture for our data set\n",
    "num_ftrs = wide_resnet101_2_.fc.in_features\n",
    "wide_resnet101_2_.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# utilize GPU if possible\n",
    "wide_resnet101_2_ = wide_resnet101_2_.to(device)\n",
    "\n",
    "# load the saved weights\n",
    "wide_resnet101_2_.load_state_dict(torch.load('RESNET_101_wide_fulldata.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Densenet-121\n",
    "\n",
    "This was not a model we submitted to Kaggle individually. However in our training and validation tests we found this model had good performance, so we included it in the ensembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densenet121_ = models.densenet121(pretrained=True)\n",
    "\n",
    "# adjust the architecture for our data set\n",
    "num_ftrs = densenet121_.classifier.in_features\n",
    "densenet121_.classifier = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# utilize GPU if possible\n",
    "densenet121_ = densenet121_.to(device)\n",
    "\n",
    "# load the saved weights\n",
    "densenet121_.load_state_dict(torch.load('DenseNet_3.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet 152\n",
    "\n",
    "This was not a model we submitted to Kaggle individually. However again, in our training and validation tests we found this model had good performance, so we included it in the ensembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet152_ = models.resnet152(pretrained=True)\n",
    "\n",
    "# adjust the architecture for our data set\n",
    "num_ftrs = resnet152_.fc.in_features\n",
    "resnet152_.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# utilize GPU if possible\n",
    "resnet152_ = resnet152_.to(device)\n",
    "\n",
    "# load the saved weights\n",
    "resnet152_.load_state_dict(torch.load('resnet152_1.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet 50\n",
    "\n",
    "This was again a good model not submitted to Kaggle individually. However we included it in the ensembler as it had good validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnext50_ = models.resnext50_32x4d(pretrained=True)\n",
    "\n",
    "# adjust the architecture for our data set\n",
    "num_ftrs = resnext50_.fc.in_features\n",
    "resnext50_.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "# utilize GPU if possible\n",
    "resnext50_ = resnext50_.to(device)\n",
    "\n",
    "# load the saved weights\n",
    "resnext50_.load_state_dict(torch.load('resnext50_32x4d_full_train (1).pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classifier\n",
    "\n",
    "- For the ensemble classifier we use a weighted voting algorithm. Ideally we would have weights per model for *each* class. These weights would be trained based on some unseen data. However, as we used all the training data in training our models, we had to find another way to weight the models.\n",
    "\n",
    "\n",
    "- The method we use is to weight each model based on its validation accuracy. This is definitely not ideal, but under the constraints we faced in terms of time, it was the most robust option. An alternative is to not have individual weights at all and simply count the predicted labels and pick the mode of these. We felt however, that this would not take into account each models performance on the test dataset, so we preferred a weighted approach.\n",
    "\n",
    "\n",
    "- The ***voting_classifier*** function takes in a list of models, $m_i$, their corresponding weights, $w_i$, and the data loader for the test data on which we want to make predictions. The voting algorithm to make prediction $\\hat x$:\n",
    "\n",
    "$$ \\hat l = \\frac{\\Sigma_1^m w_i ~ L_i}{m} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\hat l: \\text{the aggregate log softmax array}\\\\\n",
    "& w_i: \\text{weight for model} ~ i\\\\\n",
    "& L_i: \\text{array for log softmax for an batch of samples}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then to get the prediction, $\\hat x$, we simply take the maximum values per class in $\\hat l$. The code below makes it clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def voting_classifier(model_list, weights, data_loader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        models (list): A list of pretrained models that should be ensembled\n",
    "        weights (list): A list of weights to apply to each corresponding model\n",
    "        data_loader (Dataloader): The test data Dataloader\n",
    "    \"\"\"\n",
    "    for model in model_list:\n",
    "        model.eval()\n",
    "\n",
    "    y_preds, file_names = [], []\n",
    "    total_weight = sum(weights)\n",
    "    \n",
    "    # loop over each data point, get vote from each model\n",
    "    for X, y, file_paths in data_loader:\n",
    "        with torch.no_grad():\n",
    "            # make sure we can utilize the GPU if available\n",
    "            X, y = torch.from_numpy(np.array(X)).to(device), y.to(device)\n",
    "            \n",
    "            # fix filenames\n",
    "            img_names = [name.lower().split('/')[-1] for name in file_paths]\n",
    "            \n",
    "            # for each model, make a prediction for each image in the batch\n",
    "            model_preds = []\n",
    "            \n",
    "            # this will store our aggregate result\n",
    "            aggregate_prediction = torch.zeros([test_batch_size, 200]).to(device)\n",
    "            \n",
    "            # aggregate the votes for each model, using the log of the softmax\n",
    "            for model, weight in zip(model_list, weights):\n",
    "                # forward pass through the model (col: batch_element, row: number of classes)\n",
    "                a2 = model(X)\n",
    "                # get log probability and weight it\n",
    "                log_prob = F.log_softmax(a2, dim=1)*weight\n",
    "                aggregate_prediction += log_prob\n",
    "            \n",
    "            # calculate the weighted average, on which we will find the maximum per class values\n",
    "            aggregate_prediction/total_weight\n",
    "            \n",
    "            # make the predictions based off our model\n",
    "            y_pred = aggregate_prediction.max(1)[1] \n",
    "            \n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            file_names.extend(img_names)\n",
    "\n",
    "    return np.concatenate(y_preds, 0), np.array(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pick 4 models which we want to combine into the ensemble\n",
    "model_list = [densenet121_, resnet152_, resnext50_, wide_resnet101_2_]\n",
    "\n",
    "# these weights are based on the accuracy on the Kaggle leaderboard, or the validation accuracy\n",
    "#   while training the models\n",
    "weights = [0.74, 0.809, 0.807, 0.84109]\n",
    "\n",
    "assert len(model_list) == len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         filename  label\n",
      "0     test_0.jpeg    107\n",
      "1     test_1.jpeg     40\n",
      "2    test_10.jpeg     74\n",
      "3   test_100.jpeg     90\n",
      "4  test_1000.jpeg    138\n"
     ]
    }
   ],
   "source": [
    "# this function constructs and stores the csv which is submitted to kaggle\n",
    "def to_kaggle(filenames, y_preds):\n",
    "    # construct dataframe from the results\n",
    "    submission = pd.DataFrame({'filename': filenames, 'label': y_preds})\n",
    "    print(submission.head())\n",
    "    submission.to_csv('voting.csv', index=False)\n",
    "\n",
    "# # Make the predictions and construct the output dataframe\n",
    "y_preds, filenames = voting_classifier(model_list, weights, test_loader)\n",
    "\n",
    "to_kaggle(filenames, y_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
